// = Your Blog title
// See https://hubpress.gitbooks.io/hubpress-knowledgebase/content/ for information about the parameters.
// :hp-image: /covers/cover.png
// :published_at: 2019-01-31
// :hp-tags: HubPress, Blog, Open_Source,
// :hp-alt-title: My English Title

:stem: latexmath

= Learning to reconstruct

If I gave you a smoothed image:

image::mayo_convolved.png[mayo_convolved, height=400, width=400]

you could probably tell that it was generated from this image

image:mayo_phantom.png[mayo_phantom, height=400, width=400]

But what about if I instead gave you the line integrals of the image. Would you still tell it came from the image above?

image:mayo_data.png[mayo_data, height=400, width=400]

Probably not. But we will demonstrate how we can teach a computer to do it.

== An introduction to inverse problems

In inverse problems we seeks to reconstruct some parameters from indirect observations. Mathematically this can be written as finding some parameters stem:[f] that is connected to our measured data stem:[g] through the _forward model_ stem:[\mathcal{T}]:

[stem]
+++++++++++++++++
g = \mathcal{T}(f) + noise
+++++++++++++++++

Several classical image processing problems can be cast into this form, including denoising, inpainting, deconvolution and superresolution. But it is also a good model for more complicated modalities such as Computed Tomography (CT), Magnetic Resonance Imaging (MRI), etc.

== Machine learning for inverse problems

If stem:[\mathcal{T}] maps images to images, such as the convolution above, finding stem:[f] from stem:[g] using machine learning is in a sense trivial (at least on a conceptual level), and any architecture such as Denoising Autoencoders, Generative Advesarial Networks, etc could be used to learn a transformation from stem:[g] to stem:[f].

But what happens when stem:[\mathcal{T}] is more complicated? 
We'll focus on CT, where the forward model is given by the https://en.wikipedia.org/wiki/Radon_transform[Radon transform]

[stem]
+++++++++++++++++
\mathcal{T}(f)(\ell) = \int_{\ell} f(x) dx  \quad \ell \text{ is a line in } \mathbb{R}^2
+++++++++++++++++

The main complication here is that we have a global relationship between signal and data, where two widely separated points in an image can both influence a point in data. And since the forward model is global, so is the inverse problem. This means that classical machine-learning approaches based on convolutions will not work, since these are local.

One way to solve this would be to use fully-connected layers instead of convolutions, but this quickly becomes infeasible. For example, in the example above the data has stem:[1000 \cdot 1000 = 10^6] points, and the signal stem:[512 \times 512 = 2.6 \cdot 10^5]. A single fully connected layer between these would require a staggering stem:[10^6 \cdot 2.6 \cdot 10^5 = 2.6 \cdot 10^11] weights. Storing these in single precision would require one terrabyte of data. For a single layer. Obviously this is not a workable solution.

== Learned denoisers

One of the most obvious solutions to this problem is to somehow re-cast the problem back into a image-to-image problem. The most popular way of doing this is to first apply some initial reconstruction, e.g. https://en.wikipedia.org/wiki/Radon_transform#Radon_inversion_formula[Filtered Back-Projection] (FBP):

video::mayo_fbp_animation.mp4[]

Once this has been applied, we can use any standard machine-learning approach to "denoise" the result. 

