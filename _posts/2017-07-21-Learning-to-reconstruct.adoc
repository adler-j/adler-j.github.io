// = Your Blog title
// See https://hubpress.gitbooks.io/hubpress-knowledgebase/content/ for information about the parameters.
// :hp-image: /covers/cover.png
// :published_at: 2019-01-31
// :hp-tags: HubPress, Blog, Open_Source,
// :hp-alt-title: My English Title

:stem: latexmath

= Learning to reconstruct

Suppose that you were given a smoothed image:

image::mayo_convolved.png[mayo_convolved, height=300, width=300]

Then you could probably tell that it was generated from this image:

image:mayo_phantom.png[mayo_phantom, height=300, width=300]

But what about if I instead gave you the line integrals of the image:

image:mayo_data.png[mayo_data, height=300, width=300]

Would you still tell it came from the image above? Probably not. But we will demonstrate how we can teach a computer to do it.

== An introduction to inverse problems

In inverse problems we seeks to find (reconstruct) some parameters given indirect observations. Mathematically this can be stated as finding some parameters stem:[f] that is connected to our measured data stem:[g] through the _forward model_ stem:[\mathcal{T}]:

[stem]
+++++++++++++++++
g = \mathcal{T}(f) + noise
+++++++++++++++++

Several classical image processing problems can be cast into this form, including denoising, inpainting, deconvolution and superresolution. But it is also a good model for more complicated modalities such as Computed Tomography (CT), Magnetic Resonance Imaging (MRI), etc.

== Machine learning for inverse problems

If stem:[\mathcal{T}] maps images to images, such as the convolution above, finding stem:[f] from stem:[g] using machine learning is at least on a conceptual level trivial, and any standard convolutional architecture such as Autoencoders, Generative Advesarial Networks, etc could be used to learn a transformation from stem:[g] to stem:[f].

But what happens when stem:[\mathcal{T}] is more complicated? 
We'll focus on CT, where the forward model is given by the https://en.wikipedia.org/wiki/Radon_transform[Radon transform]:

[stem]
+++++++++++++++++
\mathcal{T}(f)(\ell) = \int_{\ell} f(x) dx  \quad \ell \text{ is a line in } \mathbb{R}^2
+++++++++++++++++

The main complication here is that we have a global relationship between signal and data, where two widely separated points in an image can both influence a point in data as long as they lie on the same line. And since the forward model is global, so is the inverse problem. This means that classical local machine-learning approaches based on convolutions will not work.

One way to solve this would be to use fully-connected layers instead of convolutions, but this quickly becomes infeasible. For example, in the example above the data has size stem:[1000 \times 1000 = 10^6] and the signal stem:[512 \times 512 = 2.6 \cdot 10^5]. A single fully connected layer between these would require a staggering stem:[10^6 \cdot 2.6 \cdot 10^5 = 2.6 \cdot 10^{11}] weights. Storing these in single precision would require one terabyte of data. For a single layer. 

This is obviously not a workable solution.

== Learned denoisers

One of the most obvious solutions to this problem is to somehow re-cast the problem back into a image-to-image problem. The most popular way of doing this is to first do some initial reconstruction, e.g. https://en.wikipedia.org/wiki/Radon_transform#Radon_inversion_formula[Filtered Back-Projection] (FBP):

video::mayo_fbp_animation.mp4[mayo_fbp_animation, height=309, width=600, options="autoplay,loop"]

Once this has been applied, we can use any standard machine-learning approach to "denoise" the initial reconstruction by training a neural network to take the initial reconstruction as input and return the ground truth.

Several groups have done this and the results are in fact quite remarkable:

image:learned_denoiser.png[mayo_data, height=309, width=800]

However, this method leave a sour after-taste. Sure the images certainly look better, but the only input they had was the initial reconstruction so could they truly show anything that wasn't already there? Or are they simply applying make-up?

== Learned Primal-Dual

This observation leads us to a painful conclusion: in order to get a reconstruction that contains more information than current reconstruction methods, we _need_ to work directly from raw data. But as we noted above, fully learning how to do this is practually impossible.

The solution is to take a middle way, to incorporate sufficiently much information known a-priori to make the problem tractible and then learn the rest. 

The most powerfull prior information we have is the forward operator stem:[\mathcal{T}], but it only maps images to data, how would we go from data to reconstruction? One answer is to use the https://en.wikipedia.org/wiki/Hermitian_adjoint[adjoint operator] stem:[\mathcal{T}].
