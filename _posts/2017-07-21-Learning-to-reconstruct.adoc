// = Your Blog title
// See https://hubpress.gitbooks.io/hubpress-knowledgebase/content/ for information about the parameters.
// :hp-image: /covers/cover.png
// :published_at: 2019-01-31
// :hp-tags: HubPress, Blog, Open_Source,
// :hp-alt-title: My English Title

:stem: latexmath

= Learning to reconstruct

If I gave you a smoothed image:

image::mayo_convolved.png[mayo_convolved]

would you be able to tell that it was generated from this image?

image:mayo_phantom.png[mayo_phantom]

You probably could, and you might even think that it should be possible to teach a computer how to do it.

But what about if I instead gave you the line integrals of the image. Would you still tell it came from the image above?

image:mayo_data.png[mayo_data]

Probably not. But we will demonstrate how we can teach a computer to do it.

== An introduction to inverse problems

Inverse problems refer to problems where one seeks to reconstruct some parameters  from indirect observations. Mathematically we have

[stem]
+++++++++++++++++
g = \mathcal{T}(f) + noise
+++++++++++++++++

where stem:[g] is our measured data and stem:[f] is the parameters (signal) we want to reconstruct. These are connected through the _forward model_ stem:[\mathcal{T}], which describes how some set of parameters gives rise to an observation.

Several classical image processing problems can be cast into this form, including denoising, inpainting, deconvolution and superresolution. But it is also a good model for more complicated modalities such as Computed Tomography (CT), Magnetic Resonance Imaging (MRI), etc.

== Machine learning for inverse problems

If stem:[\mathcal{T}] maps images to images, such as the convolution above, finding stem:[f] from stem:[g] using machine learning is in a sense trivial (at least on a conceptual level), and any architecture such as Denoising Autoencoders, Generative Advesarial Networks, etc could be used to learn a transformation from stem:[g] to stem:[f].

But what happens when stem:[\mathcal{T}] is more complicated? We'll focus on CT, where the forward model is given by the https://en.wikipedia.org/wiki/Radon_transform[Radon transform]

[stem]
+++++++++++++++++
\mathcal{T}(f)(\ell) = \int_{\ell} f(x) dx  \quad \ell \text{ is a line in } \mathbb{R}^2
+++++++++++++++++

The main complication here is that we have a global relationship between signal and data, where two widely separated points in an image can both influence a point in data. And since the forward model is global, so is the inverse problem. This means that classical machine-learning approaches based on convolutions will not work, since these are local.

One way to solve this would be to use fully-connected layers instead of convolutions, but this quickly becomes infeasible. For example, in the example above the data has stem:[1000 \cdot 1000 = 10^6] points, and the signal stem:[512 \times 512 = 2.6 \cdot 10^5]. A single fully connected layer between these would require a staggering stem:[10^6 \cdot 2.6 \cdot 10^5 = 2.6 \cdot 10^11] weights. Storing these in single precision would require one terrabyte of data. For a single layer. Obviously this is not a workable solution.

### Learned Primal-Dual

From the above it is obvious that fully learning the inversion is not a workable solution, and we somehow need to 

